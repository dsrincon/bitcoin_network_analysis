{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Transaction Network Analysis - Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "import blocksci\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import gc\n",
    "import os, sys\n",
    "from functools import partial\n",
    "%matplotlib notebook\n",
    "ncpu=mp.cpu_count()\n",
    "print('Number of CPUs: {}'.format(ncpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Blocks up to 2019-04-17 16:57:37:  572046 \n"
     ]
    }
   ],
   "source": [
    "# Point to parsed blockchain data\n",
    "chain = blocksci.Blockchain(\"/home/ubuntu/bitcoin\")\n",
    "types=blocksci.address_type.types\n",
    "total_blocks=chain.blocks\n",
    "print('Total Blocks up to {}:  {} '.format(total_blocks[-1].time,len(total_blocks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start_block: 341392\n",
      "End_block: 451049\n",
      "Number of Blocks: 109658 \n",
      "Highest block height: 451049\n",
      "Number of Transactions: 200 \n",
      "Number of parts: 200\n",
      "419842 Processed\n",
      "449541 Processed\n",
      "396806 Processed\n",
      "419336 Processed\n",
      "394156 Processed\n",
      "444939 Processed\n",
      "421900 Processed\n",
      "377346 Processed\n",
      "357370 Processed\n",
      "438190 Processed\n",
      "376342 Processed\n",
      "433177 Processed\n",
      "358778 Processed\n",
      "449029 Processed\n",
      "412195 Processed\n",
      "374820 Processed\n",
      "363047 Processed\n",
      "406060 Processed\n",
      "405557 Processed\n",
      "424504 Processed\n",
      "408927 Processed\n",
      "386110 Processed\n",
      "436800 Processed\n",
      "416833 Processed\n",
      "352835 Processed\n",
      "436662 Processed\n",
      "353863 Processed\n",
      "392264 Processed\n",
      "449036 Processed\n",
      "436663 Processed\n",
      "369740 Processed\n",
      "411213 Processed\n",
      "374350 Processed\n",
      "397905 Processed\n",
      "429497 Processed\n",
      "355419 Processed\n",
      "419421 Processed\n",
      "374878 Processed\n",
      "427104 Processed\n",
      "379067 Processed\n",
      "365670 Processed\n",
      "344167 Processed\n",
      "362089 Processed\n",
      "426090 Processed\n",
      "351339 Processed\n",
      "397420 Processed\n",
      "413294 Processed\n",
      "416367 Processed\n",
      "436849 Processed\n",
      "434363 Processed\n",
      "355445 Processed\n",
      "375414 Processed\n",
      "387703 Processed\n",
      "416376 Processed\n",
      "425698 Processed\n",
      "432252 Processed\n",
      "380437 Processed\n",
      "446085 Processed\n",
      "402568 Processed\n",
      "444042 Processed\n",
      "435339 Processed\n",
      "383117 Processed\n",
      "430734 Processed\n",
      "419472 Processed\n",
      "434834 Processed\n",
      "428179 Processed\n",
      "358425 Processed\n",
      "446567 Processed\n",
      "430748 Processed\n",
      "379549 Processed\n",
      "383135 Processed\n",
      "389284 Processed\n",
      "359081 Processed\n",
      "365744 Processed\n",
      "381105 Processed\n",
      "382136 Processed\n",
      "358073 Processed\n",
      "419003 Processed\n",
      "381557 Processed\n",
      "390347 Processed\n",
      "435398 Processed\n",
      "412875 Processed\n",
      "432332 Processed\n",
      "407758 Processed\n",
      "424657 Processed\n",
      "348674 Processed\n",
      "419545 Processed\n",
      "423290 Processed\n",
      "404191 Processed\n",
      "341730 Processed\n",
      "381161 Processed\n",
      "434412 Processed\n",
      "393938 Processed\n",
      "367856 Processed\n",
      "413425 Processed\n",
      "381683 Processed\n",
      "378101 Processed\n",
      "390392 Processed\n",
      "347385 Processed\n",
      "372987 Processed\n",
      "444885 Processed\n",
      "376065 Processed\n",
      "386818 Processed\n",
      "376069 Processed\n",
      "411398 Processed\n",
      "414988 Processed\n",
      "342798 Processed\n",
      "421137 Processed\n",
      "380699 Processed\n",
      "373533 Processed\n",
      "343326 Processed\n",
      "438917 Processed\n",
      "419106 Processed\n",
      "424228 Processed\n",
      "423212 Processed\n",
      "369453 Processed\n",
      "391470 Processed\n",
      "400605 Processed\n",
      "394546 Processed\n",
      "398139 Processed\n",
      "417930 Processed\n",
      "435006 Processed\n",
      "416065 Processed\n",
      "404290 Processed\n",
      "352581 Processed\n",
      "450380 Processed\n",
      "372557 Processed\n",
      "356174 Processed\n",
      "447824 Processed\n",
      "407378 Processed\n",
      "406867 Processed\n",
      "399703 Processed\n",
      "378719 Processed\n",
      "403808 Processed\n",
      "380772 Processed\n",
      "361834 Processed\n",
      "354919 Processed\n",
      "434033 Processed\n",
      "368019 Processed\n",
      "396660 Processed\n",
      "394046 Processed\n",
      "418169 Processed\n",
      "419706 Processed\n",
      "344958 Processed\n",
      "401280 Processed\n",
      "436610 Processed\n",
      "355716 Processed\n",
      "363002 Processed\n",
      "427916 Processed\n",
      "420589 Processed\n",
      "392592 Processed\n",
      "400451 Processed\n",
      "415637 Processed\n",
      "357785 Processed\n",
      "413081 Processed\n",
      "426395 Processed\n",
      "373151 Processed\n",
      "363937 Processed\n",
      "438683 Processed\n",
      "434079 Processed\n",
      "389445 Processed\n",
      "359337 Processed\n",
      "415148 Processed\n",
      "358331 Processed\n",
      "396206 Processed\n",
      "382896 Processed\n",
      "357960 Processed\n",
      "419762 Processed\n",
      "383731 Processed\n",
      "378804 Processed\n",
      "418742 Processed\n",
      "360887 Processed\n",
      "432568 Processed\n",
      "410612 Processed\n",
      "387999 Processed\n",
      "385983 Processed\n",
      "433601 Processed\n",
      "420805 Processed\n",
      "380359 Processed\n",
      "397768 Processed\n",
      "437194 Processed\n",
      "434637 Processed\n",
      "362449 Processed\n",
      "347092 Processed\n",
      "361806 Processed\n",
      "377302 Processed\n",
      "362916 Processed\n",
      "447038 Processed\n",
      "446429 Processed\n",
      "395744 Processed\n",
      "348644 Processed\n",
      "436711 Processed\n",
      "408559 Processed\n",
      "350194 Processed\n",
      "439795 Processed\n",
      "419828 Processed\n",
      "447477 Processed\n",
      "375801 Processed\n",
      "416250 Processed\n",
      "363006 Processed\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'extractions/15Tx02-200-blocks-2015_02_01_00_00_00-2017_02_01_11_59_59/15Tx02-200-blocks-2015_02_01_00_00_00-2017_02_01_11_59_59.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-29d132092796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extractions/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mextraction_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-blocks-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstart_c\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mend_c\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mextraction_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-blocks-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstart_c\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mend_c\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1524\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1628\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'extractions/15Tx02-200-blocks-2015_02_01_00_00_00-2017_02_01_11_59_59/15Tx02-200-blocks-2015_02_01_00_00_00-2017_02_01_11_59_59.csv'"
     ]
    }
   ],
   "source": [
    "#---SCRIPT: generates data for graphs in each part of the partition\n",
    "\n",
    "start='2015-02-01 00:00:00'\n",
    "end='2017-02-01 11:59:59'\n",
    "blocks=chain.range(start=start,end=end)\n",
    "sample_size=200\n",
    "partition=BchainPartition(chain,start,end,sample_size=sample_size)\n",
    "df,graphs=partition_data(partition)\n",
    "df.head()\n",
    "\n",
    "# Save to Disk\n",
    "# Generate an extraction ID\n",
    "extraction_id = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(6)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***EXTRACTION COMPLETED SUCCESSFULLY***\n"
     ]
    }
   ],
   "source": [
    "#---Save Dataframes\n",
    "\n",
    "# Create directory and save\n",
    "\n",
    "start_c=start\n",
    "start_c=start_c.replace('-','_').replace(' ','_').replace(':','_')\n",
    "end_c=end\n",
    "end_c=end_c.replace('-','_').replace(' ','_').replace(':','_')\n",
    "\n",
    "directory='extractions/'+extraction_id+'-'+str(sample_size)+'-blocks-'+start_c+'-'+end_c+'/graphs'+'/'\n",
    "\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create Filename and save\n",
    "\n",
    "filename='extractions/'+extraction_id+'-'+str(sample_size)+'-blocks-'+start_c+'-'+end_c+'/'+extraction_id+'-'+str(sample_size)+'-blocks-'+start_c+'-'+end_c+'.csv'\n",
    "df.to_csv(filename)\n",
    "\n",
    "# Graphs\n",
    "for g in graphs:\n",
    "    nx.write_gpickle(g,directory+str(g.graph['graph_id'])+'.gpickle')\n",
    "    #print('Graph: {} saved'.fromat(g.graph['graph_id']))\n",
    "\n",
    "print('\\n***EXTRACTION COMPLETED SUCCESSFULLY***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that creates a blockchain a blockchain partition (dictionary) given data range and partition type (blocks,days,weeks)\n",
    "\n",
    "class BchainPartition():\n",
    "    \n",
    "    def __init__(self,chain,start_timestamp,end_timestamp,ptype='blocks',sample_size=10):\n",
    "        blocks=chain.range(start=start_timestamp,end=end_timestamp)\n",
    "        \n",
    "        \n",
    "        self.block_h=blocks.height\n",
    "        print('Start_block: {}'.format(self.block_h[0]))\n",
    "        print('End_block: {}'.format(self.block_h[-1]))\n",
    "        \n",
    "        if sample_size>0: #Samples blocks from the \n",
    "            sample_list=list(np.random.choice(self.block_h,sample_size))\n",
    "            sample_blocks=[chain[ix_b] for ix_b in sample_list]\n",
    "            txs=[b.txes for b in sample_blocks]\n",
    "            self.partition={h:[t for t in t_l] for h,t_l in zip(sample_list,txs)}\n",
    "            self.no_parts=len(sample_blocks)\n",
    "            \n",
    "        else:  \n",
    "            if ptype=='blocks':\n",
    "                self.partition={b.height:[tx for tx in b.txes] for b in blocks}\n",
    "                self.no_parts=np.int32(len(blocks))\n",
    "                \n",
    "        \n",
    "        print('Number of Blocks: {} '.format(len(blocks)))\n",
    "        print('Highest block height: {}'.format(blocks[-1].height))\n",
    "        print('Number of Transactions: {} '.format(len(txs)))\n",
    "\n",
    "        # ***TODO: Create partition for other types of partitions (use tx.block_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes blockchain partition and outputs pandas data frame with features \n",
    "# for the graph defined by each split in the partition\n",
    "\n",
    "def partition_data(chainpartiton):\n",
    "    \n",
    "    # Dictionary with partition\n",
    "    partition=chainpartiton.partition\n",
    "    partindex=partition.keys()\n",
    "    parts=partition.values()\n",
    "    data_tuples=[]\n",
    "    graphs=[]\n",
    "    \n",
    "    print('Number of parts: {}'.format(len(partindex)))\n",
    "    \n",
    "    tuples=[(index,part) for index,part in zip(partindex,parts)]\n",
    "    \n",
    "    for t in tuples:\n",
    "        \n",
    "        data_i,columns_i,graph_i=graph_features(t,slice_type='blocks')\n",
    "        data_tuples.append((data_i,columns_i))\n",
    "        graphs.append(graph_i)\n",
    "         \n",
    "    \n",
    "    '''    \n",
    "    chunksize=len(tuples)%ncpu\n",
    "    with mp.Pool(processes=ncpu) as pool:\n",
    "        data_tuples=pool.map(graph_features,tuples,chunksize)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    columns=data_tuples[0][1] #This value is being re-written. This design choice is to mantain consistency with columns.\n",
    "    data=[i for i,j in data_tuples]\n",
    "    data=np.array(data)    \n",
    "    df=pd.DataFrame(data=data[:,:],columns=columns)\n",
    "    \n",
    "    \n",
    "    return (df,graphs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that receives a chain part (list of transactions), generates transaction graph and calculates statistics\n",
    "\n",
    "def graph_features(chain_part_tuple,slice_type='blocks'):\n",
    "    \n",
    "    index=chain_part_tuple[0]\n",
    "    chain_part=chain_part_tuple[1]\n",
    "    block_height=chain_part[-1].block_height\n",
    "    graph=block_graph(chain_part,index,slice_type)\n",
    "    nx.info(graph)\n",
    "    nodes=graph.nodes(data=True)\n",
    "    edges=graph.edges(data=True)\n",
    "    data=[index]  \n",
    "    columns=['block_height']\n",
    "    \n",
    "    # Number of Nodes\n",
    "    no_nodes=nx.number_of_nodes(graph)\n",
    "    data.append(no_nodes)\n",
    "    columns.append('no_nodes')\n",
    "    \n",
    "    # Number of Edges (address to address transactions)\n",
    "    no_edges=nx.number_of_edges(graph)\n",
    "    data.append(no_edges)\n",
    "    columns.append('no_edges')\n",
    "    \n",
    "    # Total value transacted\n",
    "    total_value=np.sum(np.array([a['value'] for n1,n2,a in edges]))\n",
    "    data.append(total_value)\n",
    "    columns.append('value_transacted')\n",
    "    \n",
    "    # Total Density\n",
    "    density=nx.density(graph)\n",
    "    data.append(density)\n",
    "    columns.append('total_density')\n",
    "    \n",
    "    # Nodes with self loops nx.loops nodes_with_selfloops(G) nodes_with_selfloops(G) \n",
    "    nodes_self=nx.number_of_selfloops(graph)\n",
    "    data.append(nodes_self)\n",
    "    columns.append('nodes_self')\n",
    "    \n",
    "    # Value of self loops nodes_with_selfloops(G)\n",
    "    values=np.array([a['value'] for n1,n2,a in nx.selfloop_edges(graph,data=True)])\n",
    "    selfloop_value=np.sum(values)\n",
    "    data.append(selfloop_value)\n",
    "    columns.append('selfloop_value')\n",
    "    \n",
    "    # Number of transactions to old addresses \n",
    "    \n",
    "    old_nodes=[n for n,a in nodes if a['block_created']<block_height]\n",
    "    edges_to_old=graph.in_edges(old_nodes,data=True)\n",
    "    data.append(len(edges_to_old))\n",
    "    columns.append('old_nodes_in')\n",
    "    \n",
    "    # Ratio of transactions to old addresses to total transactions\n",
    "    \n",
    "    ratio_oldin_totalin=len(edges_to_old)/(no_edges+1)\n",
    "    data.append(ratio_oldin_totalin)\n",
    "    columns.append('ratio_oldin_totalin')\n",
    "    \n",
    "    # Value of transactions to old addresses\n",
    "    \n",
    "    value_to_old=[a['value'] for n1,n2,a in edges_to_old]\n",
    "    data.append(np.sum(np.array(value_to_old)))\n",
    "    columns.append('value_to_old')\n",
    "    \n",
    "    # Old address density\n",
    "    \n",
    "    old_graph=nx.induced_subgraph(graph,old_nodes)\n",
    "    old_density=nx.density(old_graph)\n",
    "    data.append(old_density)\n",
    "    columns.append('old_density')\n",
    "    \n",
    "    \n",
    "    # ***TODO*** (Aggregated graph analysis)\n",
    "    \n",
    "    # Accumulated reuse\n",
    "    # Dominance (Agg graph or new vs. old dominance) \n",
    "    #https://networkx.github.io/documentation/stable/reference/algorithms/dominance.html\n",
    "    # Common ancenstors (as with dominance the address ancestor path should be proportional \n",
    "    #to the blockchain lenght if address reuse is minimal)\n",
    "    #***********\n",
    "    \n",
    "    print('{} Processed'.format(index))\n",
    "    return (data,columns,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates transaction graph for a given number transactions\n",
    "\n",
    "\n",
    "def block_graph(txs,index,slice_type):\n",
    "\n",
    "    \n",
    "    # Create graph and process \n",
    "    graph = nx.MultiDiGraph(graph_id=index,slice_type=slice_type)\n",
    "    nodes=[]\n",
    "    edges=[]\n",
    "    \n",
    "    \n",
    "    # Extract transactions information\n",
    "    \n",
    "    init_block=txs[0].block.height\n",
    "    txs_dic={tx.index:tx for tx in txs}\n",
    "    txs_ix=list(txs_dic.keys())\n",
    "    txs_ix.sort()\n",
    "   \n",
    "    \n",
    "    start_ix=txs_ix[0]\n",
    "    end_ix=txs_ix[-1]\n",
    "    \n",
    "    \n",
    "    # Generate edges to input to graph\n",
    "    \n",
    "    chunksize=len(txs)%ncpu\n",
    "   \n",
    "    #edges=chain.mapreduce_txes(extract_nodes_edges, lambda x: x , init=None, start=start_ix, end=end_ix, cpu_count=8)\n",
    "    #edges=chain.mapreduce_txes(identity, identity, init=missing_param, start=start_ix, end=end_ix, cpu_count=8)\n",
    "\n",
    "    '''\n",
    "    with mp.Pool(processes=ncpu) as pool:\n",
    "        edges=pool.map(extract_nodes_edges,txs,chunksize)\n",
    "        \n",
    "   \n",
    "    '''\n",
    "    for tx in txs:\n",
    "        edges_i,nodes_i=extract_nodes_edges(tx)\n",
    "        nodes.append(nodes_i)\n",
    "        edges.append(edges_i)\n",
    "        #print('Processed tx: {}'.format(tx.index))\n",
    "       \n",
    "    nodes=list(itertools.chain.from_iterable(nodes))\n",
    "    edges=list(itertools.chain.from_iterable(edges))\n",
    "    \n",
    "    # Input to graph\n",
    "    \n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "        \n",
    "\n",
    "    #print('Generated Graph for Block starting at:{}'.format(init_block))\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that receives a transaction and generates nodes and edges from addresses in transaction\n",
    "\n",
    "def extract_nodes_edges(transaction):\n",
    "    \n",
    "    # Initialize values and get info from transaction\n",
    "    edges=[]\n",
    "    output_value=transaction.output_value\n",
    "    block_height=transaction.block_height\n",
    "    tx_id=transaction.index\n",
    "    \n",
    "    # Get inputs, types and values\n",
    "    inputs=transaction.inputs.address\n",
    "    #input_types=transaction.inputs.address.raw_type\n",
    "    #input_block=transaction.inputs.address.first_tx.block.height\n",
    "    input_val=transaction.inputs.value\n",
    "    \n",
    "    input_nodes=[(inp.address_num,{'raw_type':inp.raw_type,'block_created':inp.first_tx.block.height})for inp in inputs]\n",
    "    \n",
    "\n",
    "    # Get outputs and types\n",
    "    outputs=transaction.outputs.address\n",
    "    #output_types=transaction.outputs.address.raw_type\n",
    "    #output_block=transaction.outputs.address.first_tx.block.height\n",
    "    output_nodes=[(out.address_num,{'raw_type':out.raw_type,'block_created':out.first_tx.block.height})for out in outputs]\n",
    "  \n",
    "    # ****TODO: Add address balance as attribute to node**** \n",
    "    \n",
    "    # Create nodes \n",
    "    \n",
    "    nodes=input_nodes+output_nodes\n",
    "    \n",
    "    # Create edges (NetworkX will automatically create nodes when given edges)\n",
    "  \n",
    "    for i in range(len(inputs)):\n",
    "        value=input_val[i]\n",
    "        prop_value=value/len(outputs)\n",
    "        \n",
    "        for o in range(len(outputs)): \n",
    "            edge=(inputs[i].address_num,outputs[o].address_num,{'value':prop_value,'tx_id':block_height})\n",
    "            edges.append(edge)\n",
    "    \n",
    "    return edges,nodes\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704f384862b91ff5d2cd170df55826f94c0d93f014d4ccf82406fabd61582434\n",
      "[(19464725, {'raw_type': 2, 'block_created': 264954}), (40165049, {'raw_type': 2, 'block_created': 309015}), (30517405, {'raw_type': 2, 'block_created': 290181})]\n",
      "[(19464725, 40165049, {'tx_id': 309015, 'value': 2500000.0}), (19464725, 30517405, {'tx_id': 309015, 'value': 2500000.0})]\n"
     ]
    }
   ],
   "source": [
    "# Test extract nodes_edges\n",
    "\n",
    "test_tx=chain.tx_with_index(41847232)\n",
    "edges,nodes=extract_nodes_edges(test_tx)\n",
    "print(test_tx.hash)\n",
    "print(nodes)\n",
    "print(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
